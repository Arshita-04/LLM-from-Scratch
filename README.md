# LLM-from-Scratch

This repository documents my complete hands-on journey of learning and implementing the core components of Large Language Models entirely from scratch. Each file represents one building block behind transformer-based architectures, progressing step-by-step from tokenization to a functional GPT-style model. The purpose of this project is to deeply understand how LLMs work under the hood rather than relying on high-level libraries. The code here breaks down the architecture into simple, readable modules so that each concept can be studied and experimented with independently.

The project covers all essential mechanisms used in modern language models. This includes self-attention, multi-head attention, feedforward networks, layer normalization, GELU activation functions, skip connections, and positional embeddings. The repository also contains experiments with tokenization using byte pair encoding and regular expressions, which are fundamental to preparing text for training. Alongside these lower-level components, the repository includes simplified GPT model versions and step-by-step training scripts used on small sample text files.

Sample datasets such as article.txt and the-verdict.txt are included to test training and inference. The goal of the collection is to provide a clear, modular, and approachable learning experience for anyone who wants to understand transformer models conceptually as well as practically. By working through the code files in order, you can build a working mental model of how GPT architectures function internally.

Overall, this repository serves as a hands-on learning playground for understanding transformers and LLMs at the code level. It is designed for students, researchers, and enthusiasts who want to go beyond high-level APIs and dive deep into the mathematical and architectural foundations behind GPT-style models. The code is modular, readable, and organized to encourage experimentation with each component of the transformer architecture.
